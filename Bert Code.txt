import os
import json
import torch
import numpy as np
import pandas as pd
from transformers import BertTokenizer, BertModel
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import scipy

# ========== CONFIG ==========
PREPOSITION = "в силу"               # Предложная конструкция
PREPOSITION_TOKEN = "силу"          # Токен, по которому будет извлекаться эмбеддинг
LANG = "russian"                    # "german" | "spanish" | "russian"

ENCODERS = {
    "russian": "DeepPavlov/rubert-base-cased",
    "german": "bert-base-german-cased",
    "spanish": "dccuchile/bert-base-spanish-wwm-cased"
}

INPUT_SOURCE = f"data/{LANG}_{PREPOSITION.replace(' ', '_')}.csv"
os.makedirs("pkl", exist_ok=True)
os.makedirs("results", exist_ok=True)

# ========== LOAD MODEL ==========
tokenizer = BertTokenizer.from_pretrained(ENCODERS[LANG])
encoder = BertModel.from_pretrained(ENCODERS[LANG], output_hidden_states=True)

# Проверка: токен должен существовать в токенизации самой конструкции
tokenized_prep = tokenizer.tokenize(PREPOSITION)
assert PREPOSITION_TOKEN in tokenized_prep, f"⚠️ '{PREPOSITION_TOKEN}' не найден в токенах: {tokenized_prep}"

# ========== LOAD DATA ==========
df = pd.read_csv(INPUT_SOURCE)
df['text'] = df['text'].astype(str).str.lower()
df['year'] = df['year'].astype('int32')

# ========== EMBEDDING FUNCTION ==========
def get_embedding(texts, model, tokenizer, prep_token):
    embs = []
    for text in texts:
        marked_text = "[CLS] " + text + " [SEP]"
        tokenized_text = tokenizer.tokenize(marked_text)
        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)[:512]
        attention_ids = [1] * len(indexed_tokens)

        tokens_tensor = torch.tensor([indexed_tokens])
        attention_tensors = torch.tensor([attention_ids])

        with torch.no_grad():
            outputs = model(input_ids=tokens_tensor, attention_mask=attention_tensors, return_dict=True)
            hidden_states = outputs.last_hidden_state

        if prep_token in tokenized_text:
            idx = tokenized_text.index(prep_token)
            vec = hidden_states[0][idx]
            embs.append(vec.numpy().tolist())
        else:
            print(f"[!] Token '{prep_token}' not found in:", marked_text)
            embs.append(np.nan)

    return embs

# ========== EMBEDDINGS ==========
df['embs'] = get_embedding(df["text"], encoder, tokenizer, PREPOSITION_TOKEN)
df.dropna(subset=['year', 'text', 'embs'], inplace=True)
df.to_pickle(f"./pkl/{LANG}_{PREPOSITION.replace(' ', '_')}.pkl")

# ========== CLUSTERING ==========
stats = {}
new_df = pd.DataFrame()

for period in [(1800, 1850), (1850, 1900), (1900, 1950), (1950, 2000)]:
    small_df = df[df['year'].between(period[0], period[1])]
    X = small_df['embs'].tolist()
    silhouette_list = []
    kmeans_models = {}

    for n_clusters in range(2, 7):
        try:
            model = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
            labels = model.predict(X)
            score = silhouette_score(X, labels)
            silhouette_list.append((n_clusters, score))
            kmeans_models[n_clusters] = model
        except Exception as e:
            print(f"[!] Clustering failed for k={n_clusters}: {e}")
            continue

    if silhouette_list:
        silhouette_list_sorted = sorted(silhouette_list, key=lambda x: x[1], reverse=True)
        best_k = silhouette_list_sorted[0][0]
        small_df["cluster"] = kmeans_models[best_k].predict(X)
        stats[period] = silhouette_list_sorted
        new_df = pd.concat([new_df, small_df], ignore_index=True)
    else:
        print(f"[!] No valid clustering for period {period}")

# ========== EXPORT ==========
new_df.sort_values(by='year', inplace=True)
new_df.drop('embs', axis=1, inplace=True)
new_df.to_csv(f"results/{LANG}_{PREPOSITION.replace(' ', '_')}_clusters.csv", index=False)

print("✅ Clustering complete.")
